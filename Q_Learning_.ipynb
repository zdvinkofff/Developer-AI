{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPDohaO04/zW3WHZEvYP7f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zdvinkofff/Developer-AI/blob/main/Q_Learning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "МОДЕЛЬ Q-LEARNING В СРЕДЕ БИБЛИОТЕКИ GYM"
      ],
      "metadata": {
        "id": "VwO8lFln5O77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.19.0\n",
        "!pip install pyglet==1.5.27\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!sudo apt-get update\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "l2E4Gt0x25vS",
        "outputId": "7730a65b-d45e-437d-b344-9d00715ab3dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.19.0 in /usr/local/lib/python3.8/dist-packages (0.19.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.19.0) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.19.0) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyglet==1.5.27 in /usr/local/lib/python3.8/dist-packages (1.5.27)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-65.6.3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\n",
            "Successfully installed setuptools-65.6.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFvIPpxi2oTv",
        "outputId": "d07c1436-6a2a-45df-834d-ddd500c80e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- using Q Learning -----\n",
            "Iteration #1 -- Total reward = -200.\n",
            "Iteration #101 -- Total reward = -200.\n",
            "Iteration #201 -- Total reward = -200.\n",
            "Iteration #301 -- Total reward = -200.\n",
            "Iteration #401 -- Total reward = -200.\n",
            "Iteration #501 -- Total reward = -200.\n",
            "Iteration #601 -- Total reward = -200.\n",
            "Iteration #701 -- Total reward = -200.\n",
            "Iteration #801 -- Total reward = -200.\n",
            "Iteration #901 -- Total reward = -200.\n",
            "Iteration #1001 -- Total reward = -200.\n",
            "Iteration #1101 -- Total reward = -200.\n",
            "Iteration #1201 -- Total reward = -200.\n",
            "Iteration #1301 -- Total reward = -200.\n",
            "Iteration #1401 -- Total reward = -200.\n",
            "Iteration #1501 -- Total reward = -200.\n",
            "Iteration #1601 -- Total reward = -200.\n",
            "Iteration #1701 -- Total reward = -200.\n",
            "Iteration #1801 -- Total reward = -200.\n",
            "Iteration #1901 -- Total reward = -200.\n",
            "Iteration #2001 -- Total reward = -200.\n",
            "Iteration #2101 -- Total reward = -200.\n",
            "Iteration #2201 -- Total reward = -200.\n",
            "Iteration #2301 -- Total reward = -200.\n",
            "Iteration #2401 -- Total reward = -200.\n",
            "Iteration #2501 -- Total reward = -200.\n",
            "Iteration #2601 -- Total reward = -200.\n",
            "Iteration #2701 -- Total reward = -200.\n",
            "Iteration #2801 -- Total reward = -200.\n",
            "Iteration #2901 -- Total reward = -200.\n",
            "Iteration #3001 -- Total reward = -200.\n",
            "Iteration #3101 -- Total reward = -200.\n",
            "Iteration #3201 -- Total reward = -200.\n",
            "Iteration #3301 -- Total reward = -200.\n",
            "Iteration #3401 -- Total reward = -200.\n",
            "Iteration #3501 -- Total reward = -200.\n",
            "Iteration #3601 -- Total reward = -200.\n",
            "Iteration #3701 -- Total reward = -200.\n",
            "Iteration #3801 -- Total reward = -200.\n",
            "Iteration #3901 -- Total reward = -200.\n",
            "Iteration #4001 -- Total reward = -200.\n",
            "Iteration #4101 -- Total reward = -200.\n",
            "Iteration #4201 -- Total reward = -200.\n",
            "Iteration #4301 -- Total reward = -200.\n",
            "Iteration #4401 -- Total reward = -200.\n",
            "Iteration #4501 -- Total reward = -200.\n",
            "Iteration #4601 -- Total reward = -200.\n",
            "Iteration #4701 -- Total reward = -200.\n",
            "Iteration #4801 -- Total reward = -200.\n",
            "Iteration #4901 -- Total reward = -200.\n",
            "Iteration #5001 -- Total reward = -200.\n",
            "Iteration #5101 -- Total reward = -200.\n",
            "Iteration #5201 -- Total reward = -200.\n",
            "Iteration #5301 -- Total reward = -200.\n",
            "Iteration #5401 -- Total reward = -200.\n",
            "Iteration #5501 -- Total reward = -200.\n",
            "Iteration #5601 -- Total reward = -200.\n",
            "Iteration #5701 -- Total reward = -200.\n",
            "Iteration #5801 -- Total reward = -200.\n",
            "Iteration #5901 -- Total reward = -200.\n",
            "Iteration #6001 -- Total reward = -200.\n",
            "Iteration #6101 -- Total reward = -200.\n",
            "Iteration #6201 -- Total reward = -200.\n",
            "Iteration #6301 -- Total reward = -200.\n",
            "Iteration #6401 -- Total reward = -200.\n",
            "Iteration #6501 -- Total reward = -200.\n",
            "Iteration #6601 -- Total reward = -200.\n",
            "Iteration #6701 -- Total reward = -200.\n",
            "Iteration #6801 -- Total reward = -200.\n",
            "Iteration #6901 -- Total reward = -200.\n",
            "Iteration #7001 -- Total reward = -200.\n",
            "Iteration #7101 -- Total reward = -200.\n",
            "Iteration #7201 -- Total reward = -200.\n",
            "Iteration #7301 -- Total reward = -200.\n",
            "Iteration #7401 -- Total reward = -200.\n",
            "Iteration #7501 -- Total reward = -200.\n",
            "Iteration #7601 -- Total reward = -200.\n",
            "Iteration #7701 -- Total reward = -200.\n",
            "Iteration #7801 -- Total reward = -200.\n",
            "Iteration #7901 -- Total reward = -200.\n",
            "Iteration #8001 -- Total reward = -198.\n",
            "Iteration #8101 -- Total reward = -200.\n",
            "Iteration #8201 -- Total reward = -200.\n",
            "Iteration #8301 -- Total reward = -200.\n",
            "Iteration #8401 -- Total reward = -200.\n",
            "Iteration #8501 -- Total reward = -200.\n",
            "Iteration #8601 -- Total reward = -200.\n",
            "Iteration #8701 -- Total reward = -200.\n",
            "Iteration #8801 -- Total reward = -200.\n",
            "Iteration #8901 -- Total reward = -200.\n",
            "Iteration #9001 -- Total reward = -200.\n",
            "Iteration #9101 -- Total reward = -200.\n",
            "Iteration #9201 -- Total reward = -200.\n",
            "Iteration #9301 -- Total reward = -200.\n",
            "Iteration #9401 -- Total reward = -200.\n",
            "Iteration #9501 -- Total reward = -200.\n",
            "Iteration #9601 -- Total reward = -200.\n",
            "Iteration #9701 -- Total reward = -200.\n",
            "Iteration #9801 -- Total reward = -200.\n",
            "Iteration #9901 -- Total reward = -200.\n",
            "Average score of solution =  -129.96\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "import os\n",
        "import random\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "import gym\n",
        "from gym import wrappers\n",
        "\n",
        "n_states = 40\n",
        "iter_max = 10000\n",
        "\n",
        "initial_lr = 1.0\n",
        "min_lr = 0.003\n",
        "gamma = 1.0\n",
        "t_max = 10000\n",
        "eps = 0.02\n",
        "\n",
        "def run_episode(env, policy=None, render=False):\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_idx = 0\n",
        "    for _ in range(t_max):\n",
        "        if render:\n",
        "            env.render()\n",
        "        if policy is None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            a,b = obs_to_state(env, obs)\n",
        "            action = policy[a][b]\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += gamma ** step_idx * reward\n",
        "        step_idx += 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "def obs_to_state(env, obs):\n",
        "    env_low = env.observation_space.low\n",
        "    env_high = env.observation_space.high\n",
        "    env_dx = (env_high - env_low) / n_states\n",
        "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
        "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
        "    return a, b\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env_name = 'MountainCar-v0'\n",
        "    env = gym.make(env_name)\n",
        "    env.seed(0)\n",
        "    np.random.seed(0)\n",
        "    print ('----- using Q Learning -----')\n",
        "    q_table = np.zeros((n_states, n_states, 3))\n",
        "    for i in range(iter_max):\n",
        "        obs = env.reset()\n",
        "        total_reward = 0\n",
        "        eta = max(min_lr, initial_lr * (0.85 ** (i//100)))\n",
        "        for j in range(t_max):\n",
        "            a, b = obs_to_state(env, obs)\n",
        "            if np.random.uniform(0, 1) < eps:\n",
        "                action = np.random.choice(env.action_space.n)\n",
        "            else:\n",
        "                logits = q_table[a][b]\n",
        "                logits_exp = np.exp(logits)\n",
        "                probs = logits_exp / np.sum(logits_exp)\n",
        "                action = np.random.choice(env.action_space.n, p=probs)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            a_, b_ = obs_to_state(env, obs)\n",
        "            q_table[a][b][action] = q_table[a][b][action] + eta * (reward + gamma *  np.max(q_table[a_][b_]) - q_table[a][b][action])\n",
        "            if done:\n",
        "                break\n",
        "        if i % 100 == 0:\n",
        "            print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward))\n",
        "    solution_policy = np.argmax(q_table, axis=2)\n",
        "    solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n",
        "    print(\"Average score of solution = \", np.mean(solution_policy_scores))\n",
        "    # Animate it\n",
        "    run_episode(env, solution_policy, True)"
      ]
    }
  ]
}